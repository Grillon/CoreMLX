# CoreMLX
## Core ML eXtensible

**Chatbot local, modulaire et Ã©volutif** â€” conÃ§u pour servir de socle libre Ã  des dÃ©monstrations MLOps, DevOps, Docker et CI/CD.

## ðŸŽ¯ Objectif

CrÃ©er un systÃ¨me dâ€™infÃ©rence local basÃ© sur un modÃ¨le prÃ©entraÃ®nÃ© encapsulÃ© dans une API compatible OpenAI, avec une interface utilisateur simple. Le tout doit Ãªtre modulaire, documentÃ©, exÃ©cutable en local et Ã©volutif vers des fonctionnalitÃ©s avancÃ©es (RAG, fine-tuning, observabilitÃ©).

## ðŸ§± Architecture (Phase 1)

- `backend/` â†’ FastAPI exposant `/v1/chat/completions`
- `model/` â†’ wrapper autour dâ€™un modÃ¨le local (e.g. `llama.cpp`, `phi-2`)
- `frontend/` â†’ Streamlit, interface de chat simple

## âœ… Avancement

Phase 1 en cours :
- [x] Structuration projet
- [x] Choix du modÃ¨le
- [x] Backend minimal
- [ ] Frontend simple
- [ ] Pitch + vidÃ©o de dÃ©monstration

---

## ðŸ“š Contexte du modÃ¨le

CoreMLX utilise le modÃ¨le **OpenHermes 2.5** (base Mistral), dÃ©veloppÃ© par **Nous Research**, un collectif initialement actif dans lâ€™Ã©cosystÃ¨me blockchain via le projet **Bittensor**.

Faute de soutien suffisant malgrÃ© la qualitÃ© de leur contribution, ils ont quittÃ© lâ€™Ã©cosystÃ¨me crypto pour se concentrer sur la crÃ©ation de modÃ¨les libres, prÃ©cis et expressifs. OpenHermes est le fruit de cette transition.

Cette trajectoire â€” quitter un environnement instable pour structurer un socle technique ouvert â€” rÃ©sonne avec lâ€™intention de ce projet.

## test du modÃ¨le

* installer llama.cpp

>Le plus simple est de prendre directement les binaires, le plus sure et souverain de le compiler Ã  partir des sources

* lancer le model

```bash
mkdir models
cd models
wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf
cd ..
llama-server -m models/openhermes-2.5-mistral-7b.Q4_K_M.gguf --port 8001
```

* tester avec curl + jq (dans votre repo de distribution favori)

```bash
prompt='Bonjour, que puis-je faire pour vous ?'
curl -s -X POST http://localhost:8001/completion \
  -H "Content-Type: application/json" \
  -d "{\"prompt\": \"$prompt\", \"n_predict\": 64}" \
  | jq -r '.content'
```



## ðŸ”“ Licence

MIT â€” rÃ©utilisable librement avec attribution.

## ðŸ”— Liens 

* model full precision  : [OpenHermes-2.5-Mistral-7B sur huggingface](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
* model quantisÃ© orientÃ© cpu: [OpenHermes-2.5-Mistral-7B-GGUF sur huggingface](TheBloke/OpenHermes-2.5-Mistral-7B-GGUF)
* llama.cpp: [llama.cpp](https://github.com/ggml-org/llama.cpp)

* (Ã  venir)

- DÃ©mo vidÃ©o
- Documentation technique
- PrÃ©sentation pÃ©dagogique

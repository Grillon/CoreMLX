# CoreMLX
üß† Infrastructure IA open source ‚Äî reproductible, modulaire, sans d√©pendance cloud.

## Core ML eXtensible

**Chatbot local, modulaire et √©volutif** ‚Äî con√ßu pour servir de socle libre √† des d√©monstrations MLOps, DevOps, Docker et CI/CD.

## üéØ Objectif

Cr√©er un syst√®me d‚Äôinf√©rence local bas√© sur un mod√®le pr√©entra√Æn√© encapsul√© dans une API compatible OpenAI, avec une interface utilisateur simple. Le tout doit √™tre modulaire, document√©, ex√©cutable en local et √©volutif vers des fonctionnalit√©s avanc√©es (RAG, fine-tuning, observabilit√©).

## üöÄ D√©marrage rapide

```bash
# Structure des scripts :
# ./scripts/
# ‚îú‚îÄ‚îÄ 01_create_venv.sh        # Cr√©e l'environnement virtuel
# ‚îú‚îÄ‚îÄ 02_install_prerequis.sh  # Installe les d√©pendances Python
# ‚îú‚îÄ‚îÄ 03_install_model.sh      # T√©l√©charge le mod√®le GGUF
# ‚îú‚îÄ‚îÄ 04_build_llamacpp.sh     # (Optionnel) Clone et compile llama.cpp
# ‚îú‚îÄ‚îÄ 05_run_model.sh          # Lance le mod√®le en local
# ‚îî‚îÄ‚îÄ mybin.sh                 # Installe les binaires dans ~/.local/bin

# Chaque script a une fonction simple. Reprise possible en cas d‚Äô√©chec.

git clone https://github.com/tonuser/CoreMLX.git
# perso je prefere ssh
# git clone git@github.com:Grillon/CoreMLX.git
cd CoreMLX

./scripts/01_create_venv.sh
./scripts/02_install_prerequis.sh
./scripts/03_install_model.sh

# (Optionnel, pour compiler et installer llama.cpp)
./scripts/04_build_llamacpp.sh
./scripts/mybin.sh -i $PWD/llama.cpp/build/bin/

# Lancement mod√®le
./scripts/05_run_model.sh

# (ou directement)
llama-server -m ./models/openhermes-2.5-mistral-7b.Q4_K_M.gguf --port 8001
```
Dans un autre terminal

```bash
source .venv/bin/activate
fastapi dev backend/app/main.py
```

On teste dans un troisi√®me terminal : 

```bash
# test llama.cpp
curl -s -X POST http://localhost:8001/completion \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Combien les humains ont-il de doigts ?", "n_predict": 64}' | jq -r '.content'

# test fastapi

curl -X 'POST' \
  'http://127.0.0.1:8000/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "messages": [
    {
      "role": "user",
      "content": "Combien de doigts les humains ont-ils?"
    }
  ]
}'
```

Ou, en Python¬†test llama.cpp :

```python
import requests

prompt = "Combien les humains ont-il de doigts ?"
response = requests.post(
    "http://localhost:8001/completion",
    json={"prompt": prompt, "n_predict": 64}
)
print(response.json()["content"])
```

en python test fastAPI :

```python
import requests

data = {
  "messages": [
    {
      "role": "user",
      "content": "Combien de doigts les humains ont-ils?"
    }
  ]
}

response = requests.post(
  "http://127.0.0.1:8000/v1/chat/completions",
  json=data
)

print(response.json())
```

Dans un autre quatri√®me terminal on lance le front : 

```bash
source .venv/bin/activate
streamlit run frontend/app.py
```

## üß± Architecture (Phase 1)

- `backend/` ‚Üí FastAPI exposant `/v1/chat/completions`
- `model/` ‚Üí wrapper autour d‚Äôun mod√®le local (e.g. `llama.cpp`, `phi-2`)
- `frontend/` ‚Üí Streamlit, interface de chat simple

## ‚úÖ Avancement

Phase 1 en cours :
- [x] Structuration projet
- [x] Choix du mod√®le
- [x] Backend minimal
- [x] Frontend simple
- [ ] Pitch + vid√©o de d√©monstration

---

## üìö Contexte du mod√®le

CoreMLX utilise le mod√®le **OpenHermes 2.5** (base Mistral), d√©velopp√© par **Nous Research**, un collectif initialement actif dans l‚Äô√©cosyst√®me blockchain via le projet **Bittensor**.

Faute de soutien suffisant malgr√© la qualit√© de leur contribution, ils ont quitt√© l‚Äô√©cosyst√®me crypto pour se concentrer sur la cr√©ation de mod√®les libres, pr√©cis et expressifs. OpenHermes est le fruit de cette transition.

Cette trajectoire ‚Äî quitter un environnement instable pour structurer un socle technique ouvert ‚Äî r√©sonne avec l‚Äôintention de ce projet.

## üß™ Test du mod√®le

* installer llama.cpp

> Le plus simple est de prendre directement les binaires. Le plus s√ªr et souverain : compiler depuis les sources.


***Si vous faite le choix de compiler alors vous pouvez utiliser les scripts setup.sh et mybin.sh***

```bash
cd scripts
./scripts/setup.sh # clone llama.cpp le build et ensuite telecharge le model q4
./scripts/mybin.sh -i $dossier_build_bin_llamacpp # installe les binaires compil√©s ; passer le chemin en 2e argument
```

* lancer le model

```bash
# si vous n'avez pas utilis√© les scripts
mkdir models
cd models
wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf
cd ..
# Ici on lance le serveur du model
llama-server -m models/openhermes-2.5-mistral-7b.Q4_K_M.gguf --port 8001
```

* tester avec curl + jq (dans votre repo de distribution favori)

```bash
prompt='Combien les humains ont-il de doigts ?'
curl -s -X POST http://localhost:8001/completion \
  -H "Content-Type: application/json" \
  -d "{\"prompt\": \"$prompt\", \"n_predict\": 64}" \
  | jq -r '.content'
```

## üîì Licence

MIT ‚Äî r√©utilisable librement avec attribution.

## üîó Liens 

* model full precision  : [OpenHermes-2.5-Mistral-7B sur huggingface](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
* model quantis√© orient√© cpu: [OpenHermes-2.5-Mistral-7B-GGUF sur HuggingFace](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF)
* llama.cpp: [llama.cpp](https://github.com/ggml-org/llama.cpp)
* fastAPI: [FastAPI](https://fastapi.tiangolo.com/)
* streamlite: [Streamlite](https://streamlit.io/#install)

## üì∫ √Ä venir

- üéûÔ∏è D√©mo vid√©o (Phase 1)
- üìò Documentation technique
- üßë Pr√©sentation p√©dagogique

## üß≠ Phase actuelle

Cette version correspond √† la **Phase 1** de CoreMLX :  
> Un chatbot local, bas√© sur un mod√®le Mistral, encapsul√© dans une API FastAPI compatible OpenAI, avec une interface Streamlit.

Les prochaines phases incluront :  
- CI/CD  
- Observabilit√©  
- RAG  
- Fine-tuning  
- Changement de mod√®le √† chaud  

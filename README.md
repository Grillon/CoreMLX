# CoreMLX
üß† Infrastructure IA open source ‚Äî reproductible, modulaire, sans d√©pendance cloud.

## Core ML eXtensible

**Chatbot local, modulaire et √©volutif** ‚Äî con√ßu pour servir de socle libre √† des d√©monstrations MLOps, DevOps, Docker et CI/CD.

## üéØ Objectif

Cr√©er un syst√®me d‚Äôinf√©rence local bas√© sur un mod√®le pr√©entra√Æn√© encapsul√© dans une API compatible OpenAI, avec une interface utilisateur simple. Le tout doit √™tre modulaire, document√©, ex√©cutable en local et √©volutif vers des fonctionnalit√©s avanc√©es (RAG, fine-tuning, observabilit√©).

## üöÄ D√©marrage rapide

```bash
# Structure des scripts :
# ./scripts/
# ‚îú‚îÄ‚îÄ 01_create_venv.sh        # Cr√©e l'environnement virtuel
# ‚îú‚îÄ‚îÄ 02_install_prerequis.sh  # Installe les d√©pendances Python
# ‚îú‚îÄ‚îÄ 03_install_model.sh      # T√©l√©charge le mod√®le GGUF
# ‚îú‚îÄ‚îÄ 04_build_llamacpp.sh     # (Optionnel) Clone et compile llama.cpp
# ‚îú‚îÄ‚îÄ 05_run_model.sh          # Lance le mod√®le en local
# ‚îî‚îÄ‚îÄ mybin.sh                 # Installe les binaires dans ~/.local/bin

# Chaque script a une fonction simple. Reprise possible en cas d‚Äô√©chec.

git clone https://github.com/Grillon/CoreMLX.git
# perso je prefere ssh
# git clone git@github.com:Grillon/CoreMLX.git
cd CoreMLX

./scripts/01_create_venv.sh
./scripts/02_install_prerequis.sh
./scripts/03_install_model.sh

# (Optionnel, pour compiler et installer llama.cpp)
./scripts/04_build_llamacpp.sh
./scripts/mybin.sh -i $PWD/llama.cpp/build/bin/

# Lancement mod√®le
./scripts/05_run_model.sh

# (ou directement)
llama-server -m ./models/openhermes-2.5-mistral-7b.Q4_K_M.gguf --port 8001
```
Dans un autre terminal

```bash
source .venv/bin/activate
fastapi dev backend/app/main.py
```

On teste dans un troisi√®me terminal : 

```bash
# test llama.cpp
curl -s -X POST http://localhost:8001/completion \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Combien les humains ont-il de doigts ?", "n_predict": 64}' | jq -r '.content'

# test fastapi

curl -X 'POST' \
  'http://127.0.0.1:8000/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "messages": [
    {
      "role": "user",
      "content": "Combien de doigts les humains ont-ils?"
    }
  ]
}'
```

Ou, en Python¬†test llama.cpp :

```python
import requests

prompt = "Combien les humains ont-il de doigts ?"
response = requests.post(
    "http://localhost:8001/completion",
    json={"prompt": prompt, "n_predict": 64}
)
print(response.json()["content"])
```

en python test fastAPI :

```python
import requests

data = {
  "messages": [
    {
      "role": "user",
      "content": "Combien de doigts les humains ont-ils?"
    }
  ]
}

response = requests.post(
  "http://127.0.0.1:8000/v1/chat/completions",
  json=data
)

print(response.json())
```

Dans un autre quatri√®me terminal on lance le front : 

```bash
source .venv/bin/activate
streamlit run frontend/app.py
```

## üß± Architecture (Phase 1)

- `backend/` ‚Üí FastAPI exposant `/v1/chat/completions`
- `model/` ‚Üí wrapper autour d‚Äôun mod√®le local (e.g. `llama.cpp`, `phi-2`)
- `frontend/` ‚Üí Streamlit, interface de chat simple

## ‚úÖ Avancement

Phase 1 en cours :
- [x] Structuration projet
- [x] Choix du mod√®le
- [x] Backend minimal
- [x] Frontend simple
- [x] Pitch + vid√©o de d√©monstration

---

## üìö Contexte du mod√®le

CoreMLX utilise le mod√®le **OpenHermes 2.5** (base Mistral), d√©velopp√© par **Nous Research**, un collectif initialement actif dans l‚Äô√©cosyst√®me blockchain via le projet **Bittensor**.

Faute de soutien suffisant malgr√© la qualit√© de leur contribution, ils ont quitt√© l‚Äô√©cosyst√®me crypto pour se concentrer sur la cr√©ation de mod√®les libres, pr√©cis et expressifs. OpenHermes est le fruit de cette transition.

Cette trajectoire ‚Äî quitter un environnement instable pour structurer un socle technique ouvert ‚Äî r√©sonne avec l‚Äôintention de ce projet.

## üß™ Configs test√©es

|Marque|mod√®le|CPU|GPU|RAM|OS|
|HP|Elitebook840 G6|Corei58thGen|None|32Go|Ubuntu 24.04.2 LTS|

## üì∫ D√©mo

[Vid√©o de pr√©sentation (1min15)](https://youtu.be/7Kb_3M7986I)

## üîì Licence

MIT ‚Äî r√©utilisable librement avec attribution.

## üîó Liens 

* model full precision  : [OpenHermes-2.5-Mistral-7B sur huggingface](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
* model quantis√© orient√© cpu: [OpenHermes-2.5-Mistral-7B-GGUF sur HuggingFace](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF)
* llama.cpp: [llama.cpp](https://github.com/ggml-org/llama.cpp)
* fastAPI: [FastAPI](https://fastapi.tiangolo.com/)
* streamlite: [Streamlite](https://streamlit.io/#install)


## üì∫ √Ä venir

- üìò Documentation technique
- üßë Pr√©sentation p√©dagogique

## üß≠ Phase actuelle

Cette version correspond √† la **Phase 1** de CoreMLX :  
> Un chatbot local, bas√© sur un mod√®le Mistral, encapsul√© dans une API FastAPI compatible OpenAI, avec une interface Streamlit.

Les prochaines phases incluront :  
- CI/CD  
- Observabilit√©  
- RAG  
- Fine-tuning  
- Changement de mod√®le √† chaud  

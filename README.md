# CoreMLX
## Core ML eXtensible

**Chatbot local, modulaire et √©volutif** ‚Äî con√ßu pour servir de socle libre √† des d√©monstrations MLOps, DevOps, Docker et CI/CD.

## üéØ Objectif

Cr√©er un syst√®me d‚Äôinf√©rence local bas√© sur un mod√®le pr√©entra√Æn√© encapsul√© dans une API compatible OpenAI, avec une interface utilisateur simple. Le tout doit √™tre modulaire, document√©, ex√©cutable en local et √©volutif vers des fonctionnalit√©s avanc√©es (RAG, fine-tuning, observabilit√©).

## üß± Architecture (Phase 1)

- `backend/` ‚Üí FastAPI exposant `/v1/chat/completions`
- `model/` ‚Üí wrapper autour d‚Äôun mod√®le local (e.g. `llama.cpp`, `phi-2`)
- `frontend/` ‚Üí Streamlit, interface de chat simple

## ‚úÖ Avancement

Phase 1 en cours :
- [x] Structuration projet
- [x] Choix du mod√®le
- [x] Backend minimal
- [x] Frontend simple
- [ ] Pitch + vid√©o de d√©monstration

---

## üìö Contexte du mod√®le

CoreMLX utilise le mod√®le **OpenHermes 2.5** (base Mistral), d√©velopp√© par **Nous Research**, un collectif initialement actif dans l‚Äô√©cosyst√®me blockchain via le projet **Bittensor**.

Faute de soutien suffisant malgr√© la qualit√© de leur contribution, ils ont quitt√© l‚Äô√©cosyst√®me crypto pour se concentrer sur la cr√©ation de mod√®les libres, pr√©cis et expressifs. OpenHermes est le fruit de cette transition.

Cette trajectoire ‚Äî quitter un environnement instable pour structurer un socle technique ouvert ‚Äî r√©sonne avec l‚Äôintention de ce projet.

## üß™ Test du mod√®le

* installer llama.cpp

> Le plus simple est de prendre directement les binaires. Le plus s√ªr et souverain : compiler depuis les sources.


***Si vous faite le choix de compiler alors vous pouvez utiliser les scripts setup.sh et mybin.sh***

```bash
cd scripts
./scripts/setup.sh # clone llama.cpp le build et ensuite telecharge le model q4
./scripts/mybin.sh -i $dossier_build_bin_llamacpp # installe les binaires compil√©s ; passer le chemin en 2e argument
```

* lancer le model

```bash
# si vous n'avez pas utilis√© les scripts
mkdir models
cd models
wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf
cd ..
# Ici on lance le serveur du model
llama-server -m models/openhermes-2.5-mistral-7b.Q4_K_M.gguf --port 8001
```

* tester avec curl + jq (dans votre repo de distribution favori)

```bash
prompt='Combien les humains ont-il de doigts ?'
curl -s -X POST http://localhost:8001/completion \
  -H "Content-Type: application/json" \
  -d "{\"prompt\": \"$prompt\", \"n_predict\": 64}" \
  | jq -r '.content'
```

## üîì Licence

MIT ‚Äî r√©utilisable librement avec attribution.

## üîó Liens 

* model full precision  : [OpenHermes-2.5-Mistral-7B sur huggingface](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
* model quantis√© orient√© cpu: [OpenHermes-2.5-Mistral-7B-GGUF sur huggingface](TheBloke/OpenHermes-2.5-Mistral-7B-GGUF)
* llama.cpp: [llama.cpp](https://github.com/ggml-org/llama.cpp)
* fastAPI: [FastAPI](https://fastapi.tiangolo.com/)
* streamlite: [Streamlite](https://streamlit.io/#install)

* (√† venir)

- D√©mo vid√©o
- Documentation technique
- Pr√©sentation p√©dagogique
